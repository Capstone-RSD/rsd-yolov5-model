{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Capstone-RSD/.github/blob/main/road1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnRIEh1-nqF3",
        "outputId": "d42dfe90-ab16-4863-c267-8dfca237c951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/workspace/'\n",
            "/yolov5/yolov5/yolov5\n",
            "/yolov5/yolov5/yolov5\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14967, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 14967 (delta 13), reused 16 (delta 6), pack-reused 14936\u001b[K\n",
            "Receiving objects: 100% (14967/14967), 13.95 MiB | 19.05 MiB/s, done.\n",
            "Resolving deltas: 100% (10277/10277), done.\n",
            "/yolov5/yolov5/yolov5/yolov5\n",
            "HEAD is now at fbe67e4 Fix `OMP_NUM_THREADS=1` for macOS (#8624)\n"
          ]
        }
      ],
      "source": [
        "# clone YOLOv5 repository\n",
        "# mount Google drive and create a \"Road\" directory\n",
        "%cd /workspace/\n",
        "!pwd\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "!git reset --hard fbe67e465375231474a2ad80a4389efc77ecff99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OroK_X4Rntuc",
        "outputId": "27ecd6d3-2903-4cac-fccc-8140e3834d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mSetup complete. Using torch 1.13.1 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3070', major=8, minor=6, total_memory=8191MB, multi_processor_count=46)\n",
            "Thu Jan 12 06:05:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.60.12    Driver Version: 527.41       CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   43C    P8    24W / 270W |    958MiB /  8192MiB |     10%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A        23      G   /Xwayland                       N/A      |\n",
            "|    0   N/A  N/A        27      G   /Xwayland                       N/A      |\n",
            "|    0   N/A  N/A        32      G   /Xwayland                       N/A      |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Using cuda for inference\n"
          ]
        }
      ],
      "source": [
        "# install dependencies as necessary\n",
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
        "import torch.cuda\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.downloads import attempt_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "# Check whether model is using GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "    !nvidia-smi\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f'Using {device} for inference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfpwix9CpLY4",
        "outputId": "cae8013b-d059-48c6-9581-20ce4a5a29fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mupload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=roboflow-yolov5\n"
          ]
        }
      ],
      "source": [
        "!pip install -q roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"roboflow-yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn3Y1kW5pTs1",
        "outputId": "10a37208-62d1-4c0e-e16a-6dd356ced883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: roboflow in /opt/conda/lib/python3.10/site-packages (0.2.24)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: idna==2.10 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: wget in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from roboflow) (6.0)\n",
            "Requirement already satisfied: glob2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.22.3)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: chardet==4.0.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.7.0.68)\n",
            "Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (9.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.6.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.0.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.38.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (2.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in pavement-cracks-1 to yolov5pytorch: 100% [127172314 / 127172314] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to pavement-cracks-1 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7840/7840 [00:00<00:00, 21085.93it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"cWbrW60dsEnAget6vKGJ\")\n",
        "project = rf.workspace(\"wzhen-vt-edu\").project(\"pavement-cracks-2wi3m\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_jAHcSD2thl",
        "outputId": "835a844f-7bb3-4823-f106-cf378b21a6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/yolov5/yolov5/yolov5/yolov5\n",
            "names:\n",
            "- alligator cracking\n",
            "- edge cracking\n",
            "- longitudinal cracking\n",
            "- patching\n",
            "- pothole\n",
            "- rutting\n",
            "- transverse cracking\n",
            "nc: 7\n",
            "train: pavement-cracks-1/train/images\n",
            "val: pavement-cracks-1/valid/images\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOtLiEVK2y-L",
        "outputId": "9cb6d57f-16f5-4e14-b95f-052df32763fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])\n",
        "    print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybW_2PuG3_6F",
        "outputId": "8b6779d9-75c8-410e-ed36-ca0cea7323e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
            "\n",
            "# Parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 0.33  # model depth multiple\n",
            "width_multiple: 0.50  # layer channel multiple\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 v6.0 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 6, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 3, C3, [1024]],\n",
            "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 v6.0 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ]
        }
      ],
      "source": [
        "%cat /workspace/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "FdmW83Z44SXa"
      },
      "outputs": [],
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SQXg3zhQ4Zth"
      },
      "outputs": [],
      "source": [
        "%%writetemplate /workspace/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tgnK8SR4l1d",
        "outputId": "6f0cb2b1-d89f-4f8a-f824-3fc93a248461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
            "\n",
            "# Parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 0.33  # model depth multiple\n",
            "width_multiple: 0.50  # layer channel multiple\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 v6.0 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 6, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 3, C3, [1024]],\n",
            "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 v6.0 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ]
        }
      ],
      "source": [
        "%cat /workspace/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHS4yjTw4qtE",
        "outputId": "c2213425-64e0-42d5-f946-3c59b7b85b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 0 ns, sys: 1e+03 ns, total: 1e+03 ns\n",
            "Wall time: 2.86 Âµs\n",
            "/workspace/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=./models/custom_yolov5s.yaml, data=/yolov5/yolov5/yolov5/yolov5/pavement-cracks-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=CUDA_LAUNCH_BLOCKING=1, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5s_results, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
            "YOLOv5 ðŸš€ v6.1-306-gfbe67e4 Python-3.10.8 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "custom_YOLOv5s summary: 283 layers, 7271276 parameters, 7271276 gradients, 17.0 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight (no decay), 70 weight, 62 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/workspace/yolov5/pavement-cracks-1/train/labels.cache' images \u001b[0m\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.8GB CUDA_LAUNCH_BLOCKING=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3423/3423\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/workspace/yolov5/pavement-cracks-1/valid/labels.cache' images an\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB CUDA_LAUNCH_BLOCKING=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 326/326 [00\u001b[0m\n",
            "Plotting labels to runs/train/yolov5s_results3/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.10 anchors/target, 0.965 Best Possible Recall (BPR). Anchors are a poor fit to dataset âš ï¸, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING: Extremely small objects found: 3 of 19608 labels are < 3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 19608 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7037: 100%|â–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9981 best possible recall, 4.22 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.293/0.704-mean/best, past_thr=0.476-mean: 26,19, 15,68, 74,19, 11,200, 68,60, 142,49, 134,96, 80,193, 183,183\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone âœ… (optional: update model *.yaml to use these anchors in the future)\n",
            "Image sizes 416 train, 416 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov5s_results3\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/99     1.64G   0.09857   0.05519   0.04983       131       416: 100%|â–ˆâ–ˆâ–ˆ\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        326        892   0.000309     0.0565   0.000253    6.8e-05\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/99        2G   0.09495   0.05681   0.04204       141       416: 100%|â–ˆâ–ˆâ–ˆ\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        326        892      0.431      0.014    0.00113   0.000249\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/99        2G   0.08865   0.05954   0.03514       143       416: 100%|â–ˆâ–ˆâ–ˆ\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        326        892      0.287     0.0505     0.0007   0.000172\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/99        2G   0.08527   0.06282   0.03264       186       416:   6%|â–Œ  \n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/yolov5/train.py\", line 642, in <module>\n",
            "    main(opt)\n",
            "  File \"/workspace/yolov5/train.py\", line 537, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/workspace/yolov5/train.py\", line 301, in train\n",
            "    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1195, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/workspace/yolov5/utils/dataloaders.py\", line 158, in __iter__\n",
            "    yield next(self.iterator)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1333, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1359, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 543, in reraise\n",
            "    raise exception\n",
            "RuntimeError: Caught RuntimeError in pin memory thread for device 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
            "    data = pin_memory(data, device)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 65, in pin_memory\n",
            "    return [pin_memory(sample, device) for sample in data]  # Backwards compatibility.\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 65, in <listcomp>\n",
            "    return [pin_memory(sample, device) for sample in data]  # Backwards compatibility.\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 53, in pin_memory\n",
            "    return data.pin_memory(device)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train yolov5s on custom data for 100 epochs\n",
        "# time its performance\n",
        "%time\n",
        "%cd /workspace/yolov5/\n",
        "!python train.py --img 416 --batch 16 --epochs 100 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "OF56fbXv47hx",
        "outputId": "cb230b14-8524-424c-e3bf-8b9ab3435f52"
      },
      "outputs": [],
      "source": [
        "# we can also output some older school graphs if the tensor board isn't working for whatever reason... \n",
        "from utils.plots import plot_results  # plot results.txt as results.png\n",
        "Image(filename='/workspace/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "4dX5WQpVLAPW",
        "outputId": "47cf14d1-80e1-47ec-b7b4-0d12f91b9082"
      },
      "outputs": [],
      "source": [
        "# first, display our ground truth data\n",
        "print(\"GROUND TRUTH TRAINING DATA:\")\n",
        "Image(filename='/workspace/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "shgCSUqqLYTz",
        "outputId": "01373a06-e942-42f8-9700-0748c3a49d8b"
      },
      "outputs": [],
      "source": [
        "# print out an augmented training example\n",
        "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
        "Image(filename='/workspace/yolov5/runs/train/yolov5s_results/train_batch0.jpg', width=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YXVuvI_NHOL",
        "outputId": "c899042b-b066-46c2-a947-10277317b567"
      },
      "outputs": [],
      "source": [
        "# trained weights are saved by default in our weights folder\n",
        "%ls runs/train/yolov5s_results/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDlNYBUbNK9w",
        "outputId": "0fe600b4-8413-4b24-a516-5b451e4f42f9"
      },
      "outputs": [],
      "source": [
        "%cd /yolov5/\n",
        "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source ./pavement-cracks-1/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8e2N-EoCNovW",
        "outputId": "96eb297d-6f95-4bf2-ed82-3fd301fcd2a6"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "c=0;\n",
        "for imageName in glob.glob('/workspace/yolov5/runs/detect/exp3/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")\n",
        "    c=c+1;\n",
        "    if(c ==10):\n",
        "      break;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEskZDP9NuDV",
        "outputId": "db002d71-5d7b-471d-b71a-58a4c0a9a4e0"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Road/yolov5/\n",
        "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source ./pavement-cracks-1/test/images --save-txt --nosave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEE4_FWmQ2pz",
        "outputId": "b7695ff5-30f0-470c-d0ed-8041297cbe9d"
      },
      "outputs": [],
      "source": [
        "#initial the model\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "ROOT = '/workspace/yolov5/'\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "#ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "weights='runs/train/yolov5s_results/weights/best.pt'\n",
        "data='pavement-cracks-1/data.yaml'\n",
        "conf_thres=0.4\n",
        "iou_thres=0.45\n",
        "imgsz=[416,416]\n",
        "\n",
        "torch.no_grad()\n",
        "# Load model\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weights, device=device, dnn=False, data=data, fp16=False)\n",
        "stride, names, pt = model.stride, model.names, model.pt\n",
        "imgsz = check_img_size(imgsz, s=stride)  # check image size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmz6QDXKTgwO",
        "outputId": "0323138b-aaef-4803-aa27-685172a98980"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "source='pavement-cracks-1/test/images'\n",
        "dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
        "bs = 1  # batch_size\n",
        "vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "# Run inference\n",
        "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
        "seen, windows, dt = 0, [], [0.0, 0.0, 0.0]\n",
        "for path, im, im0s, vid_cap, s in dataset:\n",
        "    t1 = time_sync()\n",
        "    im = torch.from_numpy(im).to(device)\n",
        "    im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "    if len(im.shape) == 3:\n",
        "        im = im[None]  # expand for batch dim\n",
        "    t2 = time_sync()\n",
        "    dt[0] += t2 - t1\n",
        "\n",
        "    # Inference\n",
        "    #visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "    pred = model(im, augment=None, visualize=False)\n",
        "    t3 = time_sync()\n",
        "    dt[1] += t3 - t2\n",
        "\n",
        "    # NMS\n",
        "    pred = non_max_suppression(pred, conf_thres, iou_thres, None, False, max_det=1000)\n",
        "    dt[2] += time_sync() - t3\n",
        "\n",
        "    # Second-stage classifier (optional)\n",
        "    # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "    seen += 1\n",
        "    p = path\n",
        "    p = Path(p)  # to Path\n",
        "    print(p)\n",
        "    det=pred[0]\n",
        "    if len(det):\n",
        "        # Rescale boxes from img_size to im0 size\n",
        "        pred2=scale_coords(im.shape[2:], det[:, :4], im0s.shape).round()\n",
        "        print(det.shape)\n",
        "        print('box: '+str(np.array(pred2.cpu())))\n",
        "        print('class: '+str(det[:,-1].cpu()))\n",
        "        print('confidence: '+str(np.array(det[:,-2].cpu())))\n",
        "        #break;\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP6FJeQeZr3r",
        "outputId": "42a36156-728a-42c7-9611-bc29682d3aec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from utils.augmentations import letterbox\n",
        "imagePath=\"/content/drive/MyDrive/Road/yolov5/pavement-cracks-1/test/images/us14--38-_jpg.rf.7067e4519392d181489b82ca5f8586c4.jpg\"\n",
        "img0 = cv2.imread(imagePath)\n",
        "\n",
        "# Padded resize\n",
        "img = letterbox(img0, imgsz, stride, auto=pt)[0]\n",
        "\n",
        "# Convert\n",
        "img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "img = np.ascontiguousarray(img)\n",
        "\n",
        "bs = 1  # batch_size\n",
        "\n",
        "# Run inference\n",
        "model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
        "seen, windows, dt = 0, [], [0.0, 0.0, 0.0]\n",
        "\n",
        "im = torch.from_numpy(img).to(device).cuda(device)\n",
        "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
        "im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "if len(im.shape) == 3:\n",
        "    im = im[None]  # expand for batch dim\n",
        "t2 = time_sync()\n",
        "dt[0] += t2 - t1\n",
        "\n",
        "# Inference\n",
        "#visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "pred = model(im, augment=None, visualize=False)\n",
        "t3 = time_sync()\n",
        "dt[1] += t3 - t2\n",
        "\n",
        "# NMS\n",
        "pred = non_max_suppression(pred, conf_thres, iou_thres, None, False, max_det=1000)\n",
        "dt[2] += time_sync() - t3\n",
        "\n",
        "# Second-stage classifier (optional)\n",
        "# pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "seen += 1\n",
        "p = path\n",
        "p = Path(p)  # to Path\n",
        "print(p)\n",
        "det=pred[0]\n",
        "if len(det):\n",
        "    # Rescale boxes from img_size to im0 size\n",
        "    pred2=scale_coords(im.shape[2:], det[:, :4], im0s.shape).round()\n",
        "    print(det.shape)\n",
        "    print('box: '+str(np.array(pred2.cpu())))\n",
        "    print('class: '+str(det[:,-1].cpu()))\n",
        "    print('confidence: '+str(np.array(det[:,-2].cpu())))\n",
        "    #break;\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "OAzGs48roYJQ",
        "outputId": "e2dfb19e-2455-4761-d1f4-cae1acc106e1"
      },
      "outputs": [],
      "source": [
        "# what 's the meaning of the numbers (answered)\n",
        "# [ xmin , ymin , xmax, ymax ]\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt \n",
        "image = cv2.imread('/workspace/yolov5/pavement-cracks-1/test/images/us14--38-_jpg.rf.7067e4519392d181489b82ca5f8586c4.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "cv2.rectangle(image, (146, 0), (263, 395), (255,0,0), 2)\n",
        "cv2.rectangle(image, (282, 320), (416, 342), (255,0,0), 2)\n",
        "cv2.rectangle(image, (0, 261), (126, 339), (255,0,0), 2)\n",
        "cv2.rectangle(image, (0, 75), (119, 94), (255,0,0), 2)\n",
        "cv2.rectangle(image, (277, 61), (414, 83), (255,0,0), 2)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJIA7rsmMT8s"
      },
      "outputs": [],
      "source": [
        "!pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import py2neo\n",
        "import numpy as np\n",
        "\n",
        "# Connect to Neo4j database\n",
        "graph = py2neo.Graph(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"soccer123\")\n",
        "\n",
        "# Export the model's state_dict to a file\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "# Node creation in Neo4j to store model data\n",
        "model_data = model.state_dict()\n",
        "for key, value in model_data.items():\n",
        "    \n",
        "    # Conver PyTorch to numpy array\n",
        "    np_array = value.numpy()\n",
        "    \n",
        "    # Creating a node to store data\n",
        "    cypher = f\"CREATE (n:ModelData {{ name: '{key}', data: {np_array.tolist()} }})\"\n",
        "    graph.run(cypher)\n",
        "\n",
        "# Verifying data\n",
        "cypher = \"MATCH (n:ModelData) RETURN n\"\n",
        "result = graph.run(cypher)\n",
        "for record in result:\n",
        "    print(record)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e337b77ef3862c8d97f93906604ed3c652de6e0587d64f35add277759bf07101"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
